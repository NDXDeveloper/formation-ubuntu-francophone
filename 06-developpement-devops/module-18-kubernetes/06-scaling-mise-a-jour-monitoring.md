# 18-6. Scaling, mise √† jour, monitoring avec Prometheus

üîù Retour √† la [Table des mati√®res](/SOMMAIRE.md)

## Introduction

Dans ce module, nous allons explorer trois aspects essentiels de la gestion d'applications dans Kubernetes : la mise √† l'√©chelle (scaling), les strat√©gies de mise √† jour, et la surveillance (monitoring) avec Prometheus. Ces comp√©tences sont indispensables pour g√©rer efficacement des applications en production sur Kubernetes.

Nous verrons comment :
- Adapter automatiquement vos applications √† la charge
- Mettre √† jour vos applications sans interruption de service
- Surveiller la sant√© et les performances de votre cluster

## Pr√©requis

Pour suivre ce tutoriel, vous devez avoir :
- Un cluster Kubernetes fonctionnel (Minikube, MicroK8s ou autre)
- kubectl configur√© pour communiquer avec votre cluster
- Des connaissances de base sur les Pods, Services et D√©ploiements dans Kubernetes

## 1. Scaling : adapter vos applications √† la demande

Le scaling consiste √† ajuster le nombre d'instances de votre application pour r√©pondre √† la demande. Kubernetes offre plusieurs m√©thodes pour mettre √† l'√©chelle vos applications.

### 1.1 Scaling manuel

Le scaling manuel est la m√©thode la plus simple pour ajuster le nombre de r√©plicas d'un d√©ploiement.

Commen√ßons par cr√©er un d√©ploiement simple pour une application web :

```bash
# Cr√©er un fichier nomm√© web-app.yaml
nano web-app.yaml
```

Contenu du fichier :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: web-app
        image: nginx:1.19
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: "100m"  # 0.1 CPU
            memory: "128Mi"
          limits:
            cpu: "200m"  # 0.2 CPU
            memory: "256Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: web-app
spec:
  selector:
    app: web-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
```

D√©ployons cette application :

```bash
kubectl apply -f web-app.yaml
```

Pour v√©rifier le d√©ploiement et les pods :

```bash
kubectl get deployments
kubectl get pods
```

#### Scaling manuel avec kubectl

Pour augmenter le nombre de r√©plicas manuellement :

```bash
# Passer de 2 √† 5 r√©plicas
kubectl scale deployment web-app --replicas=5
```

Vous pouvez v√©rifier que le nombre de pods a augment√© :

```bash
kubectl get pods
```

Vous devriez voir 5 pods `web-app` en cours d'ex√©cution.

Pour r√©duire le nombre de r√©plicas :

```bash
# R√©duire √† 3 r√©plicas
kubectl scale deployment web-app --replicas=3
```

#### Scaling manuel en modifiant le fichier YAML

Vous pouvez √©galement mettre √† jour le fichier de d√©ploiement et l'appliquer √† nouveau :

```bash
# Modifier le fichier web-app.yaml pour changer replicas: 2 en replicas: 4
nano web-app.yaml

# Appliquer les changements
kubectl apply -f web-app.yaml
```

### 1.2 Autoscaling horizontal (HPA)

L'Horizontal Pod Autoscaler (HPA) ajuste automatiquement le nombre de r√©plicas en fonction de la charge CPU, m√©moire ou autres m√©triques personnalis√©es.

Pour utiliser HPA, vous devez d'abord avoir le serveur de m√©triques install√© sur votre cluster :

```bash
# Pour Minikube
minikube addons enable metrics-server

# Pour un autre cluster, appliquez le manifeste du serveur de m√©triques
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

Cr√©ons maintenant un HPA pour notre application web :

```bash
# Cr√©er un fichier web-app-hpa.yaml
nano web-app-hpa.yaml
```

Contenu du fichier :

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

Ce HPA maintient l'utilisation moyenne du CPU √† environ 50%. Il ajustera le nombre de r√©plicas entre 2 et 10 en fonction de la charge.

Appliquez le HPA :

```bash
kubectl apply -f web-app-hpa.yaml
```

Pour v√©rifier le statut du HPA :

```bash
kubectl get hpa
```

Vous devriez voir quelque chose comme :

```
NAME          REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
web-app-hpa   Deployment/web-app   1%/50%    2         10        2          30s
```

#### Tester l'autoscaling

Pour tester l'autoscaling, nous allons g√©n√©rer une charge sur notre service :

```bash
# Cr√©er un pod temporaire pour g√©n√©rer de la charge
kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://web-app; done"
```

Dans un autre terminal, surveillez le HPA et les pods :

```bash
kubectl get hpa web-app-hpa --watch
```

Vous devriez voir l'utilisation du CPU augmenter et, apr√®s quelques minutes, le nombre de r√©plicas augmenter automatiquement.

Pour arr√™ter la g√©n√©ration de charge, utilisez `Ctrl+C` dans le terminal o√π elle s'ex√©cute.

### 1.3 Autoscaling vertical (VPA)

Contrairement √† HPA qui ajuste le nombre de pods, le Vertical Pod Autoscaler (VPA) ajuste les ressources (CPU, m√©moire) allou√©es √† chaque pod.

VPA n'est pas inclus par d√©faut dans Kubernetes. Pour l'installer :

```bash
# Cloner le d√©p√¥t VPA
git clone https://github.com/kubernetes/autoscaler.git

# Aller dans le r√©pertoire VPA
cd autoscaler/vertical-pod-autoscaler

# Installer VPA
./hack/vpa-up.sh
```

Cr√©ons un exemple de VPA :

```bash
# Cr√©er un fichier web-app-vpa.yaml
nano web-app-vpa.yaml
```

Contenu du fichier :

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: '*'
      minAllowed:
        cpu: 50m
        memory: 64Mi
      maxAllowed:
        cpu: 500m
        memory: 512Mi
```

Appliquez le VPA :

```bash
kubectl apply -f web-app-vpa.yaml
```

Le VPA analysera l'utilisation des ressources et ajustera automatiquement les requ√™tes CPU et m√©moire des pods.

## 2. Strat√©gies de mise √† jour

Kubernetes propose plusieurs strat√©gies pour mettre √† jour vos applications sans interruption de service.

### 2.1 Rolling Updates (mises √† jour progressives)

C'est la strat√©gie par d√©faut de Kubernetes. Les pods sont mis √† jour progressivement, un par un, pour assurer la disponibilit√© continue du service.

Mettons √† jour notre application web :

```bash
# Mettre √† jour l'image de notre d√©ploiement
kubectl set image deployment/web-app web-app=nginx:1.20
```

Vous pouvez surveiller la progression de la mise √† jour :

```bash
kubectl rollout status deployment/web-app
```

Vous verrez quelque chose comme :

```
Waiting for deployment "web-app" rollout to finish: 2 out of 3 new replicas have been updated...
deployment "web-app" successfully rolled out
```

La mise √† jour a remplac√© progressivement les pods, en s'assurant qu'au moins 2 pods √©taient toujours disponibles.

Vous pouvez personnaliser la strat√©gie de rolling update dans votre d√©ploiement :

```yaml
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1    # Maximum de pods indisponibles pendant la mise √† jour
      maxSurge: 1          # Maximum de pods suppl√©mentaires cr√©√©s pendant la mise √† jour
```

### 2.2 Annuler une mise √† jour (Rollback)

Si une mise √† jour pose probl√®me, vous pouvez facilement revenir √† la version pr√©c√©dente :

```bash
kubectl rollout undo deployment/web-app
```

Pour revenir √† une version sp√©cifique :

```bash
# D'abord, affichez l'historique des d√©ploiements
kubectl rollout history deployment/web-app

# Puis revenez √† une r√©vision sp√©cifique
kubectl rollout undo deployment/web-app --to-revision=2
```

### 2.3 Blue/Green Deployments

Cette strat√©gie consiste √† avoir deux environnements identiques (bleu et vert), et √† basculer le trafic de l'un √† l'autre lors d'une mise √† jour.

Voici comment l'impl√©menter dans Kubernetes :

```bash
# Cr√©er un fichier blue-green-deployment.yaml
nano blue-green-deployment.yaml
```

Contenu du fichier :

```yaml
# Version bleue (actuelle)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
      version: blue
  template:
    metadata:
      labels:
        app: web-app
        version: blue
    spec:
      containers:
      - name: web-app
        image: nginx:1.19
        ports:
        - containerPort: 80
---
# Version verte (nouvelle)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
      version: green
  template:
    metadata:
      labels:
        app: web-app
        version: green
    spec:
      containers:
      - name: web-app
        image: nginx:1.20
        ports:
        - containerPort: 80
---
# Service pointant initialement vers la version bleue
apiVersion: v1
kind: Service
metadata:
  name: web-app-service
spec:
  selector:
    app: web-app
    version: blue  # Pointe vers la version bleue
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
```

D√©ployez cette configuration :

```bash
kubectl apply -f blue-green-deployment.yaml
```

Pour basculer vers la version verte, modifiez le s√©lecteur du service :

```bash
# √âditer le service
kubectl patch service web-app-service -p '{"spec":{"selector":{"version":"green"}}}'
```

Tout le trafic est maintenant dirig√© vers la version verte. Si tout fonctionne bien, vous pouvez supprimer l'ancienne version bleue.

### 2.4 Canary Deployments

Cette strat√©gie consiste √† d√©ployer une nouvelle version pour un petit sous-ensemble d'utilisateurs avant un d√©ploiement complet.

Voici comment l'impl√©menter :

```bash
# Cr√©er un fichier canary-deployment.yaml
nano canary-deployment.yaml
```

Contenu du fichier :

```yaml
# Version stable (90% du trafic)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-stable
spec:
  replicas: 9
  selector:
    matchLabels:
      app: web-app
      version: stable
  template:
    metadata:
      labels:
        app: web-app
        version: stable
    spec:
      containers:
      - name: web-app
        image: nginx:1.19
        ports:
        - containerPort: 80
---
# Version canary (10% du trafic)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web-app
      version: canary
  template:
    metadata:
      labels:
        app: web-app
        version: canary
    spec:
      containers:
      - name: web-app
        image: nginx:1.20
        ports:
        - containerPort: 80
---
# Service qui dirige le trafic vers les deux versions
apiVersion: v1
kind: Service
metadata:
  name: web-app-service
spec:
  selector:
    app: web-app  # S√©lectionne les deux versions
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
```

D√©ployez cette configuration :

```bash
kubectl apply -f canary-deployment.yaml
```

Avec cette configuration, environ 10% du trafic (1 pod sur 10) ira vers la version canary. Si tout fonctionne bien, vous pouvez augmenter progressivement le nombre de pods canary et r√©duire les pods stables.

## 3. Monitoring avec Prometheus

Prometheus est un syst√®me de surveillance open-source qui collecte et stocke des m√©triques dans une base de donn√©es temporelle. Il est parfaitement adapt√© √† Kubernetes.

### 3.1 Installation de Prometheus avec Helm

Helm est un gestionnaire de paquets pour Kubernetes. Nous l'utiliserons pour installer Prometheus et Grafana.

Si vous n'avez pas encore Helm, installez-le :

```bash
# T√©l√©charger et installer Helm
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
```

Ajoutez le d√©p√¥t Helm de Prometheus :

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```

Cr√©ez un namespace pour Prometheus :

```bash
kubectl create namespace monitoring
```

Installez Prometheus et Grafana :

```bash
helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring
```

V√©rifiez que tout est correctement install√© :

```bash
kubectl get pods -n monitoring
```

Vous devriez voir plusieurs pods, notamment pour Prometheus, Alertmanager et Grafana.

### 3.2 Acc√®s aux interfaces

Pour acc√©der √† l'interface Prometheus, vous pouvez faire un port-forward :

```bash
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
```

Acc√©dez √† Prometheus dans votre navigateur √† l'adresse : http://localhost:9090

Pour acc√©der √† Grafana :

```bash
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
```

Acc√©dez √† Grafana dans votre navigateur √† l'adresse : http://localhost:3000

Les identifiants par d√©faut pour Grafana sont :
- Utilisateur : admin
- Mot de passe : prom-operator

### 3.3 Exploration des m√©triques Kubernetes

Dans l'interface Prometheus, vous pouvez explorer les m√©triques collect√©es. Voici quelques exemples de requ√™tes PromQL :

- Utilisation CPU par pod :
  ```
  sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod)
  ```

- Utilisation m√©moire par pod :
  ```
  sum(container_memory_usage_bytes{container!=""}) by (pod)
  ```

- Nombre de pods par n≈ìud :
  ```
  count(kube_pod_info) by (node)
  ```

### 3.4 Tableaux de bord Grafana

Grafana propose plusieurs tableaux de bord pr√©d√©finis pour Kubernetes. Explorez-les dans l'interface Grafana sous "Dashboards" > "Browse".

Les tableaux de bord les plus utiles incluent :
- Kubernetes / Compute Resources / Namespace (Pods)
- Kubernetes / Compute Resources / Node (Pods)
- Kubernetes / Compute Resources / Workload

### 3.5 Configuration des alertes

Vous pouvez configurer des alertes dans Prometheus ou Grafana pour √™tre notifi√© en cas de probl√®me.

Exemple d'alerte dans Prometheus pour une utilisation √©lev√©e de CPU :

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: high-cpu-usage
  namespace: monitoring
spec:
  groups:
  - name: cpu-usage
    rules:
    - alert: HighCpuUsage
      expr: sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod) > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.pod }} has high CPU usage"
        description: "Pod {{ $labels.pod }} has been using more than 80% CPU for 5 minutes."
```

Appliquez cette r√®gle :

```bash
kubectl apply -f high-cpu-alert.yaml
```

### 3.6 Int√©gration de vos applications

Pour que vos propres applications exposent des m√©triques √† Prometheus, vous devez les instrumenter. Voici un exemple simple pour une application Node.js :

1. Installez les d√©pendances :
   ```
   npm install prom-client express
   ```

2. Cr√©ez un point de terminaison pour les m√©triques :
   ```javascript
   const express = require('express');
   const client = require('prom-client');

   // Cr√©er un registre
   const register = new client.Registry();

   // Ajouter des m√©triques par d√©faut
   client.collectDefaultMetrics({ register });

   // Cr√©er un compteur personnalis√©
   const httpRequestsTotal = new client.Counter({
     name: 'http_requests_total',
     help: 'Total HTTP requests',
     labelNames: ['method', 'path', 'status'],
     registers: [register]
   });

   const app = express();

   // Middleware pour compter les requ√™tes
   app.use((req, res, next) => {
     res.on('finish', () => {
       httpRequestsTotal.inc({
         method: req.method,
         path: req.path,
         status: res.statusCode
       });
     });
     next();
   });

   // Route principale
   app.get('/', (req, res) => {
     res.send('Hello World!');
   });

   // Endpoint pour les m√©triques Prometheus
   app.get('/metrics', async (req, res) => {
     res.set('Content-Type', register.contentType);
     res.end(await register.metrics());
   });

   app.listen(3000, () => {
     console.log('App listening on port 3000');
   });
   ```

3. Cr√©ez un Service Monitor pour que Prometheus d√©couvre votre application :

   ```yaml
   apiVersion: monitoring.coreos.com/v1
   kind: ServiceMonitor
   metadata:
     name: my-app-monitor
     namespace: monitoring
   spec:
     selector:
       matchLabels:
         app: my-app
     endpoints:
     - port: http
       path: /metrics
       interval: 15s
   ```

   Assurez-vous que votre Service a le label `app: my-app` correspondant.

## 4. Bonnes pratiques et astuces

### 4.1 Scaling

- **D√©finissez des limites de ressources** appropri√©es pour tous vos conteneurs
- **Commencez petit** avec l'autoscaling et ajustez progressivement
- **Utilisez HPA et VPA ensemble** pour une gestion optimale des ressources
- **Testez vos applications** sous charge avant de configurer l'autoscaling

### 4.2 Mises √† jour

- **Utilisez des sondes de sant√©** (readiness/liveness probes) pour des mises √† jour plus fiables
- **Commencez avec des mises √† jour progressives** avant d'essayer des strat√©gies plus complexes
- **Automatisez les tests** apr√®s chaque d√©ploiement
- **Gardez un historique limit√©** des r√©visions pour √©conomiser des ressources

### 4.3 Monitoring

- **Collectez des m√©triques business** en plus des m√©triques techniques
- **D√©finissez des SLO/SLI** (objectifs/indicateurs de niveau de service)
- **Utilisez des dashboards par √©quipe/application**
- **Configurez des alertes utiles** mais √©vitez la fatigue d'alerte

## 5. R√©solution de probl√®mes courants

### 5.1 Probl√®mes de scaling

**Sympt√¥me** : Le HPA ne fonctionne pas correctement

**Solutions** :
- V√©rifiez que le serveur de m√©triques fonctionne : `kubectl get apiservice v1beta1.metrics.k8s.io`
- V√©rifiez les m√©triques disponibles : `kubectl top pods`
- V√©rifiez les √©v√©nements HPA : `kubectl describe hpa <nom-du-hpa>`

### 5.2 Probl√®mes de mises √† jour

**Sympt√¥me** : D√©ploiement bloqu√©

**Solutions** :
- V√©rifiez les √©v√©nements du d√©ploiement : `kubectl describe deployment <nom>`
- V√©rifiez les journaux des nouveaux pods : `kubectl logs <pod-name>`
- V√©rifiez les sondes de sant√© de vos conteneurs

### 5.3 Probl√®mes de monitoring

**Sympt√¥me** : Prometheus ne collecte pas les m√©triques de votre application

**Solutions** :
- V√©rifiez que votre application expose correctement les m√©triques : `curl http://<pod-ip>:<port>/metrics`
- V√©rifiez le ServiceMonitor : `kubectl get servicemonitor -n monitoring`
- V√©rifiez les cibles dans l'interface Prometheus (Status > Targets)

## Conclusion

Dans ce module, nous avons explor√© trois aspects essentiels de la gestion d'applications dans Kubernetes :

1. **Scaling** : Comment adapter automatiquement vos applications √† la charge, que ce soit horizontalement (plus de pods) ou verticalement (plus de ressources par pod).

2. **Strat√©gies de mise √† jour** : Comment d√©ployer de nouvelles versions de vos applications sans interruption de service, en utilisant diff√©rentes approches selon vos besoins.

3. **Monitoring avec Prometheus** : Comment surveiller la sant√© et les performances de votre cluster et de vos applications, et √™tre alert√© en cas de probl√®me.

Ces comp√©tences sont indispensables pour g√©rer efficacement des applications en production sur Kubernetes. En ma√Ætrisant ces concepts, vous pourrez assurer la fiabilit√©, la disponibilit√© et la performance de vos services.

## Exercices pratiques

1. Configurez un HPA pour une application et testez-le en g√©n√©rant de la charge
2. Impl√©mentez une strat√©gie blue/green pour une application web
3. Cr√©ez un tableau de bord Grafana personnalis√© pour surveiller vos applications
4. Instrumentez une application pour exposer des m√©triques personnalis√©es √† Prometheus
5. Configurez une alerte dans Prometheus qui vous notifie lorsqu'une application ralentit

## Ressources suppl√©mentaires

- [Documentation officielle de Kubernetes sur HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [Documentation officielle de Kubernetes sur les strat√©gies de d√©ploiement](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy)
- [Guide de requ√™tes PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [Documentation de Grafana](https://grafana.com/docs/grafana/latest/)
- [Mod√®les d'instrumentation Prometheus](https://prometheus.io/docs/practices/instrumentation/)

‚è≠Ô∏è [Introduction √† Helm](/06-developpement-devops/module-18-kubernetes/07-helm.md)
