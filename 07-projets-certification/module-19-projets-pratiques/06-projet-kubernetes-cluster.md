# 19-6. Projet Kubernetes : Cluster multi-n≈ìuds, service scaling

üîù Retour √† la [Table des mati√®res](/SOMMAIRE.md)

## Introduction

Bienvenue dans ce tutoriel consacr√© √† la cr√©ation d'un cluster Kubernetes multi-n≈ìuds et √† la mise en place du scaling de services ! Apr√®s avoir appris √† d√©ployer une application simple dans le tutoriel pr√©c√©dent, nous allons maintenant passer √† l'√©tape suivante : la cr√©ation d'un v√©ritable environnement distribu√© qui se rapproche d'une infrastructure de production.

Dans ce projet, nous allons construire un cluster Kubernetes compos√© de plusieurs n≈ìuds, puis d√©ployer une application capable de s'adapter automatiquement √† la charge de travail. Cette approche est fondamentale dans les environnements professionnels o√π la haute disponibilit√© et l'√©lasticit√© sont essentielles.

## Objectifs du projet

- Cr√©er un cluster Kubernetes multi-n≈ìuds
- Comprendre la diff√©rence entre les n≈ìuds master et worker
- Configurer une application web avec scaling horizontal et vertical
- Mettre en place un syst√®me de surveillance pour observer le scaling en action
- Tester la r√©silience du cluster face aux pannes

## Pr√©requis

- Ubuntu 22.04 LTS ou version ult√©rieure
- Minimum 8 Go de RAM et 4 c≈ìurs CPU disponibles
- Au moins 40 Go d'espace disque libre
- Une compr√©hension de base de Kubernetes (voir le tutoriel 19-5)
- Connaissances √©l√©mentaires en ligne de commande Linux

## √âtape 1 : Pr√©paration de l'environnement

Pour ce projet, nous allons utiliser kubeadm, l'outil officiel pour cr√©er et g√©rer des clusters Kubernetes. Nous simulerons un environnement multi-n≈ìuds en utilisant soit des machines virtuelles, soit des instances cloud.

### 1.1 Configuration des machines

Pour notre cluster, nous aurons besoin d'au moins 3 machines :
- 1 n≈ìud master (contr√¥le plane)
- 2 n≈ìuds worker (ou plus)

> üí° **Note pour les d√©butants** : Un n≈ìud master est comme le "cerveau" du cluster qui prend les d√©cisions, tandis que les n≈ìuds worker sont les "muscles" qui ex√©cutent r√©ellement les applications.

#### Option 1 : Utilisation de machines virtuelles locales

Si vous travaillez sur votre machine locale, vous pouvez utiliser Multipass pour cr√©er rapidement plusieurs VMs Ubuntu :

```bash
# Installation de Multipass
sudo snap install multipass

# Cr√©ation du n≈ìud master
multipass launch --name master --cpus 2 --mem 2G --disk 10G

# Cr√©ation des n≈ìuds worker
multipass launch --name worker1 --cpus 2 --mem 2G --disk 10G
multipass launch --name worker2 --cpus 2 --mem 2G --disk 10G

# V√©rification des VMs cr√©√©es
multipass list
```

#### Option 2 : Utilisation d'instances cloud (recommand√© pour une exp√©rience plus r√©aliste)

Si vous disposez d'un compte chez un fournisseur cloud (AWS, GCP, DigitalOcean, etc.), vous pouvez cr√©er 3 instances avec les caract√©ristiques minimales suivantes :
- 2 vCPU
- 2 Go RAM
- Ubuntu 22.04 LTS
- Ouverture des ports n√©cessaires dans le groupe de s√©curit√© (22/SSH, 6443/Kubernetes API, 10250/Kubelet)

### 1.2 Pr√©paration des n≈ìuds

Sur **toutes les machines** (master et workers), effectuez les op√©rations suivantes pour pr√©parer l'installation de Kubernetes :

```bash
# Si vous utilisez Multipass, connectez-vous √† chaque VM :
# multipass shell master
# multipass shell worker1
# multipass shell worker2

# Mise √† jour du syst√®me
sudo apt update
sudo apt upgrade -y

# Installation des pr√©requis
sudo apt install -y apt-transport-https ca-certificates curl software-properties-common

# Configuration du module de noyau n√©cessaire pour Kubernetes
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# Configuration r√©seau pour Kubernetes
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system

# D√©sactivation du swap (n√©cessaire pour Kubernetes)
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
```

### 1.3 Installation de containerd (runtime de conteneurs)

Sur **toutes les machines**, installez containerd comme runtime de conteneurs :

```bash
# Installation de containerd
sudo apt install -y containerd

# Cr√©ation du r√©pertoire de configuration
sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml > /dev/null

# Modification de la configuration pour utiliser systemd comme cgroup driver
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml

# Red√©marrage du service
sudo systemctl restart containerd
sudo systemctl enable containerd
```

### 1.4 Installation des composants Kubernetes

Sur **toutes les machines**, installez kubeadm, kubelet et kubectl :

```bash
# Ajout de la cl√© GPG et du d√©p√¥t Kubernetes
sudo curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

# Mise √† jour des d√©p√¥ts et installation des composants
sudo apt update
sudo apt install -y kubelet kubeadm kubectl

# Blocage des mises √† jour automatiques
sudo apt-mark hold kubelet kubeadm kubectl
```

## √âtape 2 : Initialisation du cluster Kubernetes

### 2.1 Initialisation du n≈ìud master

Sur le **n≈ìud master uniquement**, initialisez le cluster :

```bash
# Initialisation du cluster (notez l'adresse IP du n≈ìud master)
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=<ADRESSE_IP_MASTER>

# Note : Remplacez <ADRESSE_IP_MASTER> par l'adresse IP de votre n≈ìud master
# Si vous utilisez Multipass, obtenez-la avec : multipass info master | grep IPv4
```

Apr√®s l'initialisation, vous verrez un message de succ√®s avec des instructions. Suivez ces instructions pour configurer kubectl :

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

> ‚ö†Ô∏è **Important** : Gardez pr√©cieusement la commande `kubeadm join` affich√©e √† la fin de l'initialisation, nous en aurons besoin pour connecter les n≈ìuds worker.

### 2.2 Installation d'un plugin de r√©seau

Sur le **n≈ìud master**, installez Flannel comme plugin de r√©seau :

```bash
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
```

### 2.3 Connexion des n≈ìuds worker

Sur **chaque n≈ìud worker**, ex√©cutez la commande `kubeadm join` que vous avez obtenue lors de l'initialisation du master. Elle ressemble √† ceci :

```bash
sudo kubeadm join <ADRESSE_IP_MASTER>:6443 --token <TOKEN> --discovery-token-ca-cert-hash sha256:<HASH>
```

Si vous avez perdu cette commande, vous pouvez la r√©g√©n√©rer sur le n≈ìud master :

```bash
# G√©n√©ration d'un nouveau token
kubeadm token create --print-join-command
```

### 2.4 V√©rification du cluster

Sur le **n≈ìud master**, v√©rifiez que tous les n≈ìuds sont correctement connect√©s :

```bash
kubectl get nodes
```

Vous devriez voir tous vos n≈ìuds (master et workers) avec le statut "Ready" :

```
NAME      STATUS   ROLES           AGE     VERSION
master    Ready    control-plane   10m     v1.26.1
worker1   Ready    <none>          5m      v1.26.1
worker2   Ready    <none>          5m      v1.26.1
```

## √âtape 3 : D√©ploiement d'une application avec scaling

### 3.1 Cr√©ation d'un d√©ploiement

Nous allons maintenant d√©ployer une application web simple qui sera automatiquement r√©partie sur nos n≈ìuds workers.

Sur le **n≈ìud master**, cr√©ez un fichier pour le d√©ploiement :

```bash
nano webapp-deployment.yaml
```

Ajoutez le contenu suivant :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
```

> üí° **Explication** : Ce d√©ploiement cr√©e 3 r√©pliques (pods) de notre application, chacune avec des limites de ressources d√©finies. La valeur "100m" pour le CPU signifie 100 millicores, soit 10% d'un c≈ìur CPU.

Appliquez le d√©ploiement :

```bash
kubectl apply -f webapp-deployment.yaml
```

### 3.2 Cr√©ation d'un service pour exposer l'application

Cr√©ez un fichier pour le service :

```bash
nano webapp-service.yaml
```

Ajoutez le contenu suivant :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 80
  type: NodePort
```

Appliquez le service :

```bash
kubectl apply -f webapp-service.yaml
```

### 3.3 V√©rification du d√©ploiement

V√©rifiez que les pods sont bien r√©partis sur les diff√©rents n≈ìuds :

```bash
kubectl get pods -o wide
```

Vous devriez voir vos pods r√©partis sur worker1 et worker2.

Pour acc√©der √† l'application, trouvez le port attribu√© au service :

```bash
kubectl get service webapp-service
```

Notez le port NodePort (g√©n√©ralement dans la plage 30000-32767). Vous pouvez maintenant acc√©der √† l'application via l'adresse IP de n'importe quel n≈ìud worker sur ce port :

```
http://<ADRESSE_IP_WORKER>:<NODE_PORT>
```

## √âtape 4 : Configuration du scaling horizontal

Le scaling horizontal consiste √† augmenter ou diminuer automatiquement le nombre de pods en fonction de la charge.

### 4.1 Installation de Metrics Server

Pour que Kubernetes puisse surveiller l'utilisation des ressources, installons Metrics Server :

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

Si vous rencontrez des probl√®mes avec Metrics Server dans votre environnement de test, vous pouvez modifier son d√©ploiement pour d√©sactiver la v√©rification TLS :

```bash
kubectl edit deployment metrics-server -n kube-system
```

Dans l'√©diteur, ajoutez `--kubelet-insecure-tls` aux arguments du conteneur :

```yaml
spec:
  containers:
  - args:
    - --kubelet-insecure-tls  # Ajoutez cette ligne
    - --cert-dir=/tmp
    - --secure-port=4443
    # ...
```

V√©rifiez que Metrics Server fonctionne :

```bash
kubectl top nodes
```

### 4.2 Cr√©ation d'un HorizontalPodAutoscaler (HPA)

Cr√©ez un fichier pour le HPA :

```bash
nano webapp-hpa.yaml
```

Ajoutez le contenu suivant :

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

> üí° **Explication** : Ce HPA maintient entre 3 et 10 r√©pliques de notre application. Il augmente le nombre de pods lorsque l'utilisation moyenne du CPU d√©passe 50% de la capacit√© demand√©e.

Appliquez le HPA :

```bash
kubectl apply -f webapp-hpa.yaml
```

### 4.3 Test du scaling horizontal

Pour tester le scaling horizontal, nous allons g√©n√©rer une charge sur notre application. Cr√©ons un pod qui va envoyer des requ√™tes √† notre service :

```bash
kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://webapp-service; done"
```

Dans un autre terminal, observez le comportement du HPA :

```bash
kubectl get hpa webapp-hpa --watch
```

Vous devriez voir le nombre de r√©pliques augmenter progressivement √† mesure que l'utilisation du CPU augmente.

Pour arr√™ter le test, appuyez sur Ctrl+C dans le terminal o√π vous avez lanc√© le pod de charge.

## √âtape 5 : Configuration du scaling vertical des n≈ìuds (cluster autoscaler)

Dans un environnement cloud, vous pouvez √©galement configurer le scaling automatique des n≈ìuds. Nous allons simuler ce comportement en ajoutant manuellement un troisi√®me n≈ìud worker.

### 5.1 Ajout d'un nouveau n≈ìud worker

Si vous utilisez Multipass, cr√©ez une nouvelle VM :

```bash
multipass launch --name worker3 --cpus 2 --mem 2G --disk 10G
```

R√©p√©tez les √©tapes 1.2 √† 1.4 pour ce nouveau n≈ìud, puis ex√©cutez la commande `kubeadm join` que vous avez utilis√©e pr√©c√©demment.

### 5.2 V√©rification du nouveau n≈ìud

Sur le n≈ìud master, v√©rifiez que le nouveau n≈ìud a bien rejoint le cluster :

```bash
kubectl get nodes
```

### 5.3 D√©ploiement d'une application avec anti-affinit√©

Pour d√©montrer comment Kubernetes peut √©quilibrer la charge sur les n≈ìuds, cr√©ons un d√©ploiement avec des r√®gles d'anti-affinit√© qui favorisent la r√©partition des pods sur diff√©rents n≈ìuds :

```bash
nano balanced-deployment.yaml
```

Ajoutez le contenu suivant :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: balanced-app
  labels:
    app: balanced-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: balanced-app
  template:
    metadata:
      labels:
        app: balanced-app
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - balanced-app
              topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

> üí° **Explication** : Cette configuration utilise `podAntiAffinity` pour que Kubernetes essaie de placer les pods sur des n≈ìuds diff√©rents autant que possible.

Appliquez le d√©ploiement :

```bash
kubectl apply -f balanced-deployment.yaml
```

### 5.4 V√©rification de la r√©partition

V√©rifiez comment les pods sont r√©partis sur les n≈ìuds :

```bash
kubectl get pods -l app=balanced-app -o wide
```

Vous devriez voir les pods distribu√©s de mani√®re √©quilibr√©e sur vos trois n≈ìuds worker.

## √âtape 6 : Mise en place d'un tableau de bord de surveillance

### 6.1 Installation du tableau de bord Kubernetes

Le tableau de bord Kubernetes est une interface web qui permet de surveiller et g√©rer votre cluster :

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
```

### 6.2 Cr√©ation d'un compte de service pour l'acc√®s au tableau de bord

Cr√©ez un fichier pour le compte de service :

```bash
nano dashboard-adminuser.yaml
```

Ajoutez le contenu suivant :

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
```

Appliquez la configuration :

```bash
kubectl apply -f dashboard-adminuser.yaml
```

### 6.3 Obtention du token d'acc√®s

```bash
kubectl -n kubernetes-dashboard create token admin-user
```

Copiez le token g√©n√©r√©, vous en aurez besoin pour vous connecter au tableau de bord.

### 6.4 Acc√®s au tableau de bord

Pour acc√©der au tableau de bord en toute s√©curit√©, utilisez le proxy kubectl :

```bash
kubectl proxy
```

Puis ouvrez cette URL dans votre navigateur :
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

Connectez-vous en utilisant le token g√©n√©r√© pr√©c√©demment.

## √âtape 7 : Test de r√©silience du cluster

Une des grandes forces de Kubernetes est sa capacit√© √† maintenir l'√©tat souhait√© m√™me en cas de panne.

### 7.1 Test de panne d'un pod

Supprimez manuellement un pod pour voir comment Kubernetes le recr√©e automatiquement :

```bash
# Listez les pods
kubectl get pods

# Supprimez un pod (remplacez <POD_NAME> par le nom d'un de vos pods webapp)
kubectl delete pod <POD_NAME>

# V√©rifiez que Kubernetes a cr√©√© un nouveau pod
kubectl get pods
```

### 7.2 Test de panne d'un n≈ìud

> ‚ö†Ô∏è **Note** : Cette partie est simul√©e. Dans un environnement r√©el, nous arr√™terions physiquement un n≈ìud.

Videz un n≈ìud worker pour simuler une panne :

```bash
# Marquez un n≈ìud comme non-programmable (remplacez <WORKER_NODE> par le nom d'un de vos n≈ìuds worker)
kubectl cordon <WORKER_NODE>

# √âvacuez tous les pods de ce n≈ìud
kubectl drain <WORKER_NODE> --ignore-daemonsets --delete-emptydir-data

# V√©rifiez que les pods ont √©t√© redistribu√©s sur les autres n≈ìuds
kubectl get pods -o wide
```

Observez comment les pods qui √©taient sur ce n≈ìud sont automatiquement recr√©√©s sur les n≈ìuds restants.

Pour remettre le n≈ìud en service :

```bash
kubectl uncordon <WORKER_NODE>
```

## √âtape 8 : Nettoyage et conclusion

### 8.1 Suppression des ressources cr√©√©es

Lorsque vous avez termin√© avec votre cluster de test, vous pouvez supprimer les ressources cr√©√©es :

```bash
kubectl delete -f webapp-hpa.yaml
kubectl delete -f webapp-service.yaml
kubectl delete -f webapp-deployment.yaml
kubectl delete -f balanced-deployment.yaml
kubectl delete -f dashboard-adminuser.yaml
```

### 8.2 D√©mant√®lement du cluster (optionnel)

Si vous souhaitez d√©manteler compl√®tement votre cluster :

Sur les n≈ìuds worker :

```bash
sudo kubeadm reset
```

Sur le n≈ìud master :

```bash
sudo kubeadm reset
```

Si vous avez utilis√© Multipass, vous pouvez supprimer les VMs :

```bash
multipass delete master worker1 worker2 worker3
multipass purge
```

## Conclusion

F√©licitations ! Vous avez r√©ussi √† :

1. Cr√©er un cluster Kubernetes multi-n≈ìuds
2. D√©ployer une application web r√©partie sur plusieurs n≈ìuds
3. Configurer le scaling horizontal automatique
4. Mettre en place un tableau de bord de surveillance
5. Tester la r√©silience du cluster face aux pannes

Ce projet vous a donn√© un aper√ßu d'un environnement Kubernetes proche de la production. Vous avez maintenant les comp√©tences de base pour g√©rer des applications distribu√©es √† grande √©chelle.

## Pour aller plus loin

- **Configuration d'un stockage persistant** : Explorez les PersistentVolumes pour les applications avec √©tat
- **Mise en place d'un registre d'images priv√©** : Apprenez √† g√©rer vos propres images de conteneurs
- **D√©ploiement d'une base de donn√©es distribu√©e** : Essayez de d√©ployer MongoDB ou PostgreSQL avec r√©plication
- **Service Mesh** : D√©couvrez Istio pour le routage avanc√©, le contr√¥le d'acc√®s et l'observabilit√©
- **GitOps** : Explorez des outils comme ArgoCD ou Flux pour automatiser les d√©ploiements depuis Git

## Ressources utiles

- [Documentation officielle de Kubernetes](https://kubernetes.io/docs/home/)
- [Guide de la haute disponibilit√© Kubernetes](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/)
- [Patterns d'architecture Kubernetes](https://k8spatterns.io/)
- [Kubernetes : Le guide complet](https://www.kubernetesbestpractices.com/)
- [Hands-on Kubernetes Workshops](https://kube.academy/)

‚è≠Ô∏è [Module 20 ‚Äì √âvaluation finale](/07-projets-certification/module-20-evaluation-finale/README.md)
